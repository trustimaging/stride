{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collected-blend",
   "metadata": {},
   "source": [
    "# Abstractions for solving optimisation problems\n",
    "\n",
    "Techniques such as ultrasound computed tomography or optoacoustic tomography are most generally formulated as mathematical optimisation problems, which are solved numerically by using local gradient-based methods like gradient descent. \n",
    "\n",
    "Abstractions are then needed that allow us to pose our optimisation problems, calculate gradients\n",
    "of those problems with respect to the relevant parameters, and then apply these gradients through some local\n",
    "optimisation algorithm.\n",
    "\n",
    "In this notebook, we will introduce these abstractions from the point of view of Stride."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-cloud",
   "metadata": {},
   "source": [
    "## Mathematical basics - Gradient calculation\n",
    "\n",
    "We will first review some of the mathematical basics behind these abstractions. Feel free to skip to the next section if you are not interested in diving into the math!\n",
    "\n",
    "Consider a continuously differentiable function $f(\\mathbf{y}) = \\left\\langle \\hat{f}(\\mathbf{y}), 1 \\right\\rangle$ with some bilinear form $\\left\\langle \\alpha, \\beta \\right\\rangle$. We know that the directional derivative of $f(\\mathbf{y})$ with respect to $\\mathbf{y}$ is,\n",
    "\n",
    "$$\n",
    "\\nabla_\\mathbf{y} f(\\mathbf{y}) \\delta\\mathbf{y} \n",
    "    = \\left\\langle \\nabla_\\mathbf{y} \\hat{f}(\\mathbf{y}) \\delta\\mathbf{y}, 1 \\right\\rangle\n",
    "    = \\left\\langle \\nabla_\\mathbf{y} \\hat{f}(\\mathbf{y}), \\delta\\mathbf{y} \\right\\rangle\n",
    "$$\n",
    "\n",
    "Consider now that $\\mathbf{y} = \\mathbf{g}(\\mathbf{z})$ is another continuously differentiable function. Then the derivative of $f(\\mathbf{y})$ with respect to $\\mathbf{z}$ is,\n",
    "\n",
    "$$\n",
    "    \\nabla_\\mathbf{z} f(\\mathbf{y}) \\delta\\mathbf{z} \n",
    "    = \\left\\langle \\nabla_\\mathbf{y} \\hat{f}(\\mathbf{y}), \\delta\\mathbf{y} \\right\\rangle\n",
    "    = \\left\\langle \\nabla_\\mathbf{y} \\hat{f}(\\mathbf{y}), \\nabla_\\mathbf{z} \\mathbf{g}(\\mathbf{z}) \\delta\\mathbf{z} \\right\\rangle\n",
    "$$\n",
    "\n",
    "by virtue of the product rule. Let's now introduce the concept of the adjoint of an operator: given an operator $D\\cdot$, its adjoint is $D^*\\cdot$, defined so that $\\left\\langle a, Db  \\right\\rangle = \\left\\langle b, D^*a  \\right\\rangle$. Then, we can rewrite the expression as,\n",
    "\n",
    "$$\n",
    "    \\nabla_\\mathbf{z} f(\\mathbf{y}) \\delta\\mathbf{z} \n",
    "    = \\left\\langle \\nabla_\\mathbf{y} \\hat{f}(\\mathbf{y}), \\nabla_\\mathbf{z} \\mathbf{g}(\\mathbf{z}) \\delta\\mathbf{z} \\right\\rangle\n",
    "    = \\left\\langle \\nabla_\\mathbf{z}^* \\mathbf{g}(\\mathbf{z}) \\nabla_\\mathbf{y} \\hat{f}(\\mathbf{y}), \\delta\\mathbf{z} \\right\\rangle\n",
    "$$\n",
    "\n",
    "That is, the derivative of function $f(\\mathbf{y})$ with respect to $\\mathbf{z}$ can be calculated by finding the derivative of $\\hat{f}(\\mathbf{y})$ with respect to its input $\\mathbf{y}$ and then applying the adjoint of the Jacobian of $\\mathbf{g}(\\mathbf{z})$ on the result. In the discrete case, this is equivalent to the Jacobian-vector product. \n",
    "\n",
    "Similarly, if we added a third function $\\mathbf{z} = \\mathbf{h}(\\mathbf{x})$, then the same result could be obtained for the derivative of $f(\\mathbf{y})$ with respect to $\\mathbf{x}$,\n",
    "\n",
    "$$\n",
    "    \\nabla_\\mathbf{x} f(\\mathbf{y}) \\delta\\mathbf{x} \n",
    "    = \\left\\langle \\nabla_\\mathbf{z}^* \\mathbf{g}(\\mathbf{z}) \\nabla_\\mathbf{y} \\hat{f}(\\mathbf{y}), \\delta\\mathbf{z} \\right\\rangle \\\\\n",
    "    = \\left\\langle \\nabla_\\mathbf{z}^* \\mathbf{g}(\\mathbf{z}) \\nabla_\\mathbf{y} \\hat{f}(\\mathbf{y}), \\nabla_\\mathbf{x} \\mathbf{h}(\\mathbf{x}) \\delta\\mathbf{x} \\right\\rangle \\\\\n",
    "    = \\left\\langle \\nabla_\\mathbf{x}^* \\mathbf{h}(\\mathbf{x}) \\nabla_\\mathbf{z}^* \\mathbf{g}(\\mathbf{z}) \\nabla_\\mathbf{y} \\hat{f}(\\mathbf{y}), \\delta\\mathbf{x} \\right\\rangle\n",
    "$$\n",
    "\n",
    "and the same procedure could be followed for any arbitrary chain of functions for whose inputs we wanted to calculate a derivative. This procedure, known as the adjoint method or backpropagation in the field of machine learning, is effectively the reverse mode that automatic differentiation libraries provide to calculate derivatives and the core abstraction used in Stride."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-appraisal",
   "metadata": {},
   "source": [
    "## Gradient calculation in Stride\n",
    "\n",
    "Stride considers all components in the optimisation problem, from partial differential equations to objective functions, as mathematical functions that can be arbitrarily composed, and whose derivative can be automatically calculated. In Stride, each of these functions is a ``stride.Operator`` object, where their inputs and outputs are ``stride.Variable`` objects.\n",
    "\n",
    "Let's see how this works by creating a ``stride.Scalar`` object ``x``, which inherits from ``stride.Variable``, and using Stride to calculate the gradient of some arbitrary functions with respect to to ``x``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "manual-genius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stride import Scalar\n",
    "from stride_examples import f, g, h\n",
    "\n",
    "x = Scalar(name=\"x\", needs_grad=True)\n",
    "z = await h(x)\n",
    "y = await g(z)\n",
    "w = await f(y)\n",
    "\n",
    "w.clear_grad()\n",
    "await w.adjoint()\n",
    "# The gradient is now in \"x.grad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-solution",
   "metadata": {},
   "source": [
    "When each ``stride.Operator`` is called, it is immediately applied on its inputs to generate some outputs. At the same time, these outputs keep a record of the chain of calls that have led to them within a directed acyclic graph. When ``w.adjoint()`` is called, this graph is traversed from the root ``w`` to the leaf ``x``, calculating the gradient in the process. Only the leaves for which the flag ``needs_grad`` is set to ``True`` will have their gradient computed, which will be stored in the internal buffer of the variable ``x.grad``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-belly",
   "metadata": {},
   "source": [
    "## Mathematical basics - PDE-constrained optimisation\n",
    "\n",
    "Now, we proceed to apply these general abstractions to find the gradient of a more practical optimisation problem. This section will contain some more math, so feel free to jump to the next section if you are not interested. \n",
    "\n",
    "Consider the PDE-constrained optimisation problem,\n",
    "\n",
    "$$\n",
    "    \\mathbf{m}^* = argmin_{\\mathbf{m}} J(\\mathbf{u}, \\mathbf{m}) = \n",
    "    argmin_{\\mathbf{m}} \\left\\langle \\hat{J}(\\mathbf{u}, \\mathbf{m}), 1 \\right\\rangle\n",
    "$$\n",
    "$$\n",
    "    s.t.\\; \\mathbf{L}(\\mathbf{u},\\mathbf{m}) = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "given some scalar objective function or loss function $J(\\mathbf{u}, \\mathbf{m})$ and some PDE $\\mathbf{L}(\\mathbf{u},\\mathbf{m}) = \\mathbf{0}$, for some vector of state variables $\\mathbf{u}$ and a vector of design variables $\\mathbf{m}$. \n",
    "\n",
    "If we consider $\\mathbf{L}(\\mathbf{u},\\mathbf{m})$ to be an adequate, continuously differentiable function in some neighbourhood of $\\mathbf{m}$, we can apply the implicit function theorem. Then $\\mathbf{L}(\\mathbf{u},\\mathbf{m}) = \\mathbf{0}$ has a unique continuously differentiable solution $\\mathbf{u}(\\mathbf{m})$, whose derivative is given by the solution of,\n",
    "\n",
    "$$\n",
    "    \\nabla_\\mathbf{u}\\mathbf{L}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}) \\nabla_\\mathbf{m}\\mathbf{u}(\\mathbf{m}) \\delta\\mathbf{m} +\n",
    "    \\nabla_\\mathbf{m}\\mathbf{L}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}) \\delta\\mathbf{m} = \\mathbf{0}\n",
    "$$\n",
    "$$\n",
    "    \\nabla_\\mathbf{m}\\mathbf{u}(\\mathbf{m})\\delta\\mathbf{m} = - \\nabla_\\mathbf{u}\\mathbf{L}^{-1}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m})\n",
    "    \\nabla_\\mathbf{m}\\mathbf{L}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}) \\delta\\mathbf{m}\n",
    "$$\n",
    "\n",
    "We can then define a reduced objective $F(\\mathbf{m}) = J(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}) = \\left\\langle \\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), 1 \\right\\rangle$, and we can take its derivative with respect to $\\mathbf{m}$,\n",
    "\n",
    "$$\n",
    "    \\nabla_\\mathbf{m} F(\\mathbf{m})(\\delta \\mathbf{m}) = \n",
    "    \\left\\langle \\nabla_\\mathbf{u}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), \\nabla_\\mathbf{m}\\mathbf{u}(\\mathbf{m})\\delta\\mathbf{m} \\right\\rangle \n",
    "    + \\left\\langle \\nabla_\\mathbf{m}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), \\delta \\mathbf{m} \\right\\rangle \n",
    "    = \\left\\langle \\nabla_\\mathbf{m}^*\\mathbf{u}(\\mathbf{m}) \\nabla_\\mathbf{u}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), \\delta\\mathbf{m} \\right\\rangle \n",
    "    + \\left\\langle \\nabla_\\mathbf{m}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), \\delta \\mathbf{m} \\right\\rangle\n",
    "$$\n",
    "\n",
    "After some substitutions we obtain,\n",
    "\n",
    "$$\n",
    "    \\nabla_\\mathbf{m} F(\\mathbf{m})(\\delta \\mathbf{m}) = \n",
    "    \\left\\langle \\nabla_\\mathbf{m}^*\\mathbf{u}(\\mathbf{m}) \\nabla_\\mathbf{u}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), \\delta\\mathbf{m} \\right\\rangle \n",
    "    + \\left\\langle \\nabla_\\mathbf{m}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), \\delta \\mathbf{m} \\right\\rangle \n",
    "    = - \\left\\langle \\nabla_\\mathbf{m}\\mathbf{L}^*(\\mathbf{u}(\\mathbf{m}), \\mathbf{m})\n",
    "    \\nabla_\\mathbf{u}\\mathbf{L}^{-*}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}) \\right. \n",
    "     \\left. \\nabla_\\mathbf{u}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), \\delta\\mathbf{m} \\right\\rangle \n",
    "    + \\left\\langle \\nabla_\\mathbf{m}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), \\delta \\mathbf{m} \\right\\rangle \n",
    "    = \\left\\langle \\nabla_\\mathbf{m}\\mathbf{L}^*(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}) \\mathbf{w}(\\mathbf{m}), \\delta\\mathbf{m} \\right\\rangle \n",
    "    + \\left\\langle \\nabla_\\mathbf{m}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m}), \\delta \\mathbf{m} \\right\\rangle\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}(\\mathbf{m})$ is the solution of the adjoint PDE,\n",
    "\n",
    "$$\n",
    "    \\mathbf{w}(\\mathbf{m}) = \n",
    "    - \\nabla_\\mathbf{u}\\mathbf{L}^{-*} (\\mathbf{u}(\\mathbf{m}), \\mathbf{m})\n",
    "    \\nabla_\\mathbf{u}\\hat{J}(\\mathbf{u}(\\mathbf{m}), \\mathbf{m})\n",
    "$$\n",
    "\n",
    "In this optimisation problem, both $\\mathbf{L}(\\mathbf{u}, \\mathbf{m})$ and $J(\\mathbf{u}, \\mathbf{m})$ would be ``stride.Operator`` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-rendering",
   "metadata": {},
   "source": [
    "## Stride operators\n",
    "\n",
    "Adding new functions to Stride requires defining a new ``stride.Operator`` subclass that implement two methods, ``forward`` and ``adjoint``.\n",
    "\n",
    "Let's see how we can do this for a function that represents the PDE ``L`` and one that represents a loss function ``J``. We will then use them to calculate the gradient with respect to the ``stride.Scalar`` ``m``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "psychological-distributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scalar"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stride import Operator, Scalar\n",
    "\n",
    "class L(Operator):\n",
    "    \"\"\"\n",
    "    L represents a partial differential equation and its adjoint.\n",
    "    \n",
    "    \"\"\"\n",
    "    def forward(self, m):\n",
    "        u = m.alike()\n",
    "        # Compute wave equation solution\n",
    "        return u\n",
    "        \n",
    "    def adjoint(self, grad_u, m):\n",
    "        grad_m = m.alike()\n",
    "        # Calculate derivative wrt to m\n",
    "        # applying adjoint on grad_u\n",
    "        return grad_m\n",
    "        \n",
    "class J(Operator):\n",
    "    \"\"\"\n",
    "    J represents a loss function or functional.\n",
    "    \n",
    "    \"\"\"\n",
    "    def forward(self, u, m):\n",
    "        loss = Scalar()\n",
    "        # Calculate loss value\n",
    "        return loss\n",
    "        \n",
    "    def adjoint(self, grad_loss, u, m):\n",
    "        grad_u = u.alike()\n",
    "        # Calculate the derivative wrt u\n",
    "        grad_m = m.alike()\n",
    "        # Calculate the derivative wrt m\n",
    "        return grad_u, grad_m\n",
    "        \n",
    "# Create the design parameters\n",
    "m = Scalar(name=\"m\")\n",
    "m.needs_grad = True\n",
    "\n",
    "# Instantiate the operators\n",
    "l = L()\n",
    "j = J()\n",
    "\n",
    "# Apply to calculate gradient\n",
    "u = await l(m)\n",
    "loss = await j(u, m)\n",
    "\n",
    "m.clear_grad()\n",
    "await loss.adjoint()\n",
    "# The gradient is now in \"m.grad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-prediction",
   "metadata": {},
   "source": [
    "## Applying the gradients\n",
    "\n",
    "The abstractions presented allow us to intuitively pose optimisation problems and calculate derivatives of an objective function with respect to the parameters of interest. However, in order to solve the problem, we have to apply this derivative to update our guess of the parameters and repeat the procedure iteratively until we are satisfied with the final result.\n",
    "\n",
    "Stride provides local optimisers of type ``stride.Optimiser`` that determine how parameters should be updated given an available derivative. \n",
    "\n",
    "For our previous example, we can then apply a step of gradient descent in the direction of our calculated derivative by using the class ``stride.GradientDescent``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sunset-blake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating variable m,\n",
      "\t grad before processing in range [2.627646e-17, 2.627646e-17]\n",
      "\t grad after processing in range [1.313823e-19, 1.313823e-19]\n",
      "\t variable range before update [2.627646e-17, 2.627646e-17]\n",
      "\t taking final update step of 1.000000e+00 [unclipped step of 1.000000e+00]\n",
      "\t variable range after update [2.614507e-17, 2.614507e-17]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stride import GradientDescent\n",
    "\n",
    "optimiser = GradientDescent(m, step_size=1.)\n",
    "await optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-reducing",
   "metadata": {},
   "source": [
    "In order to iterate through the optimisation procedure, we could use a standard Python ``for`` loop. However, we also provide in Stride a ``stride.OptimisationLoop`` to use in these cases, which will help structure and keep track of the optimisation process. \n",
    "\n",
    "Iterations in Stride are grouped together in blocks, with the ``stride.OptimisationLoop`` containing multiple blocks and each block containing multiple iterations. Partitioning the inversion in this way allows us to divide the optimisation more easily into logical units that share some characteristics. For instance, in FWI it is common to gradually introduce frequency information into the inversion to better condition the optimisation. In this case, it would make sense to assign one block to each frequency band, and run that band for some desired number of iterations. \n",
    "\n",
    "Let's add an ``stride.OptimisationLoop`` around our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infinite-nevada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating variable m,\n",
      "\t grad before processing in range [5.242153e-17, 5.242153e-17]\n",
      "\t grad after processing in range [1.307254e-19, 1.307254e-19]\n",
      "\t variable range before update [2.614507e-17, 2.614507e-17]\n",
      "\t taking final update step of 1.000000e+00 [unclipped step of 1.000000e+00]\n",
      "\t variable range after update [2.601435e-17, 2.601435e-17]\n",
      "Updating variable m,\n",
      "\t grad before processing in range [7.843588e-17, 7.843588e-17]\n",
      "\t grad after processing in range [1.300717e-19, 1.300717e-19]\n",
      "\t variable range before update [2.601435e-17, 2.601435e-17]\n",
      "\t taking final update step of 1.000000e+00 [unclipped step of 1.000000e+00]\n",
      "\t variable range after update [2.588428e-17, 2.588428e-17]\n",
      "Updating variable m,\n",
      "\t grad before processing in range [1.043202e-16, 1.043202e-16]\n",
      "\t grad after processing in range [1.294214e-19, 1.294214e-19]\n",
      "\t variable range before update [2.588428e-17, 2.588428e-17]\n",
      "\t taking final update step of 1.000000e+00 [unclipped step of 1.000000e+00]\n",
      "\t variable range after update [2.575486e-17, 2.575486e-17]\n",
      "Updating variable m,\n",
      "\t grad before processing in range [1.300750e-16, 1.300750e-16]\n",
      "\t grad after processing in range [1.287743e-19, 1.287743e-19]\n",
      "\t variable range before update [2.575486e-17, 2.575486e-17]\n",
      "\t taking final update step of 1.000000e+00 [unclipped step of 1.000000e+00]\n",
      "\t variable range after update [2.562608e-17, 2.562608e-17]\n",
      "Updating variable m,\n",
      "\t grad before processing in range [1.557011e-16, 1.557011e-16]\n",
      "\t grad after processing in range [1.281304e-19, 1.281304e-19]\n",
      "\t variable range before update [2.562608e-17, 2.562608e-17]\n",
      "\t taking final update step of 1.000000e+00 [unclipped step of 1.000000e+00]\n",
      "\t variable range after update [2.549795e-17, 2.549795e-17]\n",
      "Updating variable m,\n",
      "\t grad before processing in range [1.811990e-16, 1.811990e-16]\n",
      "\t grad after processing in range [1.274898e-19, 1.274898e-19]\n",
      "\t variable range before update [2.549795e-17, 2.549795e-17]\n",
      "\t taking final update step of 1.000000e+00 [unclipped step of 1.000000e+00]\n",
      "\t variable range after update [2.537046e-17, 2.537046e-17]\n"
     ]
    }
   ],
   "source": [
    "from stride import OptimisationLoop\n",
    "\n",
    "opt_loop = OptimisationLoop()\n",
    "\n",
    "num_blocks = 2\n",
    "num_iters = 3\n",
    "\n",
    "for block in opt_loop.blocks(num_blocks):\n",
    "    for iteration in block.iterations(num_iters):\n",
    "        m.clear_grad()\n",
    "        \n",
    "        u = await l(m)\n",
    "        loss = await j(u, m)\n",
    "        await loss.adjoint()\n",
    "        \n",
    "        await optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-fraud",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
